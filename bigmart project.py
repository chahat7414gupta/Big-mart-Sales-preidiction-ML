# -*- coding: utf-8 -*-
"""bigmart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j74dsdD4aP29gITBIcuBiqyAX3Sa4BCH
"""

print ()

print("chahat gupta")

print ("BIG MART SALEA PRIDICTION USING MACHINE LEARNING")

import numpy as np #play with numbers
import matplotlib.pyplot as plt # for plotting graph 
import seaborn as sns #for plotting graphs
import pandas as pd #for mean median mode 
from google.colab import drive
drive.mount('/content/gdrive',force_remount=True)

path1 = '/content/gdrive/MyDrive/Colab Notebooks/traning and testing data/'
path2 = '/content/gdrive/MyDrive/Colab Notebooks/traning and testing data/'
df_train=pd.read_csv(path1+'Train.csv')
#df_train=pd.read_csv(r'https://drive.google.com/file/d/1LHt5KNbM4c4HVorsVLS6ruz-eL1ERaIS/view?usp=sharing\train.csv')
df_test=pd.read_csv(path2+'Test.csv')
#df_test=pd.read_csv(r'https://drive.google.com/file/d/1uvxWKUFaLkyxRsXtoauVOxfWAPrZ6SFa/view?usp=sharing\test.csv')

df_train.head() #to show data

df_train.isnull().sum()

df_train.shape

df_test.shape

# don't drop the null values... fill the null values with some data

df_test.isnull().sum()

df_train.info() #soln of to cut the null values :- mean (numerical) & mode(catergiecial)

df_train.describe()

df_train['Item_Weight'].describe()

df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean(),inplace=True)

df_test['Item_Weight'].fillna(df_test['Item_Weight'].mean(),inplace=True)

df_train.isnull().sum()

df_train['Item_Weight'].describe()

df_train['Outlet_Size']

df_train['Outlet_Size'].value_counts()

df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0],inplace=True)

df_test['Outlet_Size'].fillna(df_test['Outlet_Size'].mode()[0],inplace=True)

df_train

df_train.isnull().sum()

df_test.isnull().sum()

# SELECTING FEATURE BASED ON RECIQREMENTS

df_train.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)
df_test.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)

df_train

df_test

#!pip install -U dtale

sns.distplot(df_train['Item_Outlet_Sales'])

sns.boxplot(df_train['Item_Weight'])

sns.countplot(x='Outlet_Size',data=df_train)

sns.countplot(df_train['Outlet_Type'])

!pip install klib

import klib

# klib.describe - functions for visualizing datasets
klib.cat_plot(df_train) # returns a visualization of the number and frequency of categorical function

! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

klib.corr_mat(df_train)

klib.dist_plot(df_train) # returns a distribution plot for every numeric feature

klib.missingval_plot(df_train)  # returns a figure containing information about missing values

klib.corr_plot(df_train)  # returns a color-encoded heatmap, ideal for correlations

from pandas_profiling import ProfileReport

profile = ProfileReport(df_train, title="Pandas Profiling Report")

profile

plt.figure(figsize=(10,5))
sns.heatmap(df_train.corr(),annot=True)
plt.show()



"""# Data cleaning using Klib liberary"""

# klib.clean - functions for cleaning datasets
klib.data_cleaning(df_train) # performs datacleaning (drop duplicates & empty rows/cols, adjust dtypes,...)

klib.clean_column_names(df_train) # cleans and standardizes column names, also called inside data_cleaning()

df_train.info()

df_train=klib.convert_datatypes(df_train) # converts existing to more efficient dtypes, also called inside data_cleaning()
df_train.info()

klib.mv_col_handling(df_train)

"""# Preprocessing Task before Model Building

1.   Label Encoding
"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

df_train['item_fat_content']= le.fit_transform(df_train['item_fat_content'])
df_train['item_type']= le.fit_transform(df_train['item_type'])
df_train['outlet_size']= le.fit_transform(df_train['outlet_size'])
df_train['outlet_location_type']= le.fit_transform(df_train['outlet_location_type'])
df_train['outlet_type']= le.fit_transform(df_train['outlet_type'])

df_train



"""
2.  Splitting our data into train and test

"""

X=df_train.drop('item_outlet_sales',axis=1)

Y=df_train['item_outlet_sales']

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=101, test_size=0.2)



"""
3.   Standarization


"""

X.describe()

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()

X_train_std= sc.fit_transform(X_train)

X_test_std= sc.transform(X_test)

X_train_std

X_test_std

Y_train

Y_test

import joblib

joblib.dump(sc,r'C:\Users\chaha\.vscode\app.py\models\sc.sav')



"""# Model Building"""

from sklearn.linear_model import LinearRegression
lr= LinearRegression()

lr.fit(X_train_std,Y_train)

X_test.head()

Y_pred_lr=lr.predict(X_test_std)

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

print(r2_score(Y_test,Y_pred_lr))
print(mean_absolute_error(Y_test,Y_pred_lr))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_lr)))

joblib.dump(lr,r'D:\Python37\Projects\iNeuron Intership Projects\BigMart-Sales\models\lr.sav')

from sklearn.ensemble import RandomForestRegressor
rf= RandomForestRegressor(n_estimators=1000)

rf.fit(X_train_std,Y_train)

Y_pred_rf= rf.predict(X_test_std)

print(r2_score(Y_test,Y_pred_rf))
print(mean_absolute_error(Y_test,Y_pred_rf))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_rf)))



"""# Hyper Parameter Tuning"""

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = RandomForestRegressor()
n_estimators = [10, 100, 1000]
max_depth=range(1,31)
min_samples_leaf=np.linspace(0.1, 1.0)
max_features=["auto", "sqrt", "log2"]
min_samples_split=np.linspace(0.1, 1.0, 10)

# define grid search
grid = dict(n_estimators=n_estimators)

#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=101)

grid_search_forest = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, 
                           scoring='r2',error_score=0,verbose=2,cv=2)

grid_search_forest.fit(X_train_std, Y_train)

# summarize results
print(f"Best: {grid_search_forest.best_score_:.3f} using {grid_search_forest.best_params_}")
means = grid_search_forest.cv_results_['mean_test_score']
stds = grid_search_forest.cv_results_['std_test_score']
params = grid_search_forest.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

grid_search_forest.best_params_

grid_search_forest.best_score_

Y_pred_rf_grid=grid_search_forest.predict(X_test_std)

r2_score(Y_test,Y_pred_rf_grid)



"""# Save your model"""

import joblib

joblib.dump(grid_search_forest,r'C:\Users\chaha\.vscode\app.py\random_forest_grid.sav')

model=joblib.load(r'C:\Users\chaha\.vscode\app.py\random_forest_grid.sav')

